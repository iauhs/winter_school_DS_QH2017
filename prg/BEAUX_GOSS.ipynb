{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from sklearn import decomposition\n",
    "import sklearn.feature_extraction.text as text\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincent/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_string(string):\n",
    "    th_regexp = re.compile('[a-zA-Z\\d]*-th')\n",
    "    citation_regexp = re.compile('[a-zA-Z]+[\\d]+')\n",
    "    digit_regexp = re.compile('[\\d]+')\n",
    "    double_whitespace_regexp = re.compile('\\s[\\s]+')\n",
    "    non_alphabet_regexp = re.compile('[\\W]')\n",
    "    one_letter_regexp = re.compile('\\s[a-zA-Z]\\s')\n",
    "    two_letters_regexp = re.compile('\\s[a-zA-Z][a-zA-Z]\\s')   \n",
    "    \n",
    "    \n",
    "    \n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "    \n",
    "    verbs_tags_set = set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    \n",
    "    resulted_tokens = [token for (token, tag) in tagged_tokens if tag not in verbs_tags_set]\n",
    "    \n",
    "    for (token,tag) in tagged_tokens:\n",
    "        if tag not in verbs_tags_set:\n",
    "            resulted_tokens.append(token)\n",
    "            \n",
    "    result = \" \".join(resulted_tokens)\n",
    "\n",
    "\n",
    "    result = string\n",
    "    result = th_regexp.sub('', result)\n",
    "#     result = citation_regexp.sub('', result)\n",
    "    result = digit_regexp.sub('', result)\n",
    "    result = non_alphabet_regexp.sub(' ', result)\n",
    "    result = one_letter_regexp.sub(' ', result)\n",
    "    result = two_letters_regexp.sub(' ', result)\n",
    "    result = double_whitespace_regexp.sub(' ', result)    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../input/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "corpus = [preprocess_string(element[5]) for element in node_info]\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "# TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# importance_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.0001, ngram_range=(1,1))\n",
    "# TFIDF_matrix_sparse = importance_vectorizer.fit_transform(corpus)\n",
    "# num_topics = 20\n",
    "# U, S, V = svds(TFIDF_matrix_sparse, k=num_topics)\n",
    "# TOPIC_matrix = np.dot(TFIDF_matrix, V.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "num_topics = 150\n",
    "_,_, V = svds(TFIDF_matrix, k=num_topics)\n",
    "TOPIC_matrix = TFIDF_matrix.dot(V.transpose())\n",
    "del V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "\n",
    "def get_features(data_fname, info_fname, portion, with_labels=True, node_info = None, tfidf_mat=None,\n",
    "                 topic_mat=None, graph=None):\n",
    "    with open(data_fname, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data_set = list(reader)\n",
    "\n",
    "    data_set = [element[0].split(\" \") for element in data_set]\n",
    "    \n",
    "\n",
    "#     with open(info_fname, \"r\") as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         node_info  = list(reader)\n",
    "        \n",
    "        \n",
    "    if portion == 1.0:\n",
    "        print \"one portion\", portion\n",
    "        to_keep = range(len(data_set))\n",
    "    else:                       \n",
    "        #to test code we select sample\n",
    "        print \"portion\", portion\n",
    "        random.seed(22)\n",
    "        to_keep = random.sample(range(len(data_set)), k=int(round(len(data_set)*portion)))            \n",
    "    \n",
    "    data_set = [data_set[i] for i in to_keep]\n",
    "    \n",
    "    perm = [i for i in to_keep]\n",
    "    \n",
    "    \n",
    "    valid_ids=set()\n",
    "    for element in data_set:\n",
    "        valid_ids.add(element[0])\n",
    "        valid_ids.add(element[1])\n",
    "        \n",
    "    if node_info is None:\n",
    "        tmp=[element for element in node_info if element[0] in valid_ids ]\n",
    "        node_info=tmp\n",
    "        del tmp\n",
    "\n",
    "\n",
    "\n",
    "    IDs = []\n",
    "    ID_pos={}\n",
    "    for element in node_info:\n",
    "        ID_pos[element[0]]=len(IDs)\n",
    "        IDs.append(element[0])\n",
    "        \n",
    "    print \"build graph\"\n",
    "    if graph is None:\n",
    "        graph = nx.Graph()\n",
    "        edges = []    \n",
    "        nodes = set()\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            nodes.add(source)\n",
    "            nodes.add(target)\n",
    "        \n",
    "            if with_labels:\n",
    "                if data_set[i][2] == \"1\":\n",
    "                    edges.append((source, target))\n",
    "            else:\n",
    "                edges.append((source, target))\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "        graph.add_nodes_from(nodes)\n",
    "        graph.add_edges_from(edges)\n",
    "    else:\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "    \n",
    "    clustering_coeff = nx.clustering(graph)\n",
    "    pr = nx.pagerank(graph, alpha=0.9)\n",
    "    jaccard_coeff = nx.jaccard_coefficient(graph, all_edges)\n",
    "    adamic_index = nx.adamic_adar_index(graph, all_edges)\n",
    "    pref_attach = nx.preferential_attachment(graph, all_edges)\n",
    "    ress_alloc = nx.resource_allocation_index(graph, all_edges)\n",
    "#     soundarajan = nx.cn_soundarajan_hopcroft(graph, all_edges)\n",
    "#     community = nx.within_inter_cluster(graph, all_edges)\n",
    "    hubs,_=nx.hits(graph,max_iter=1000)\n",
    "\n",
    "    # graph features\n",
    "    neighbor_count_source = []\n",
    "    neighbor_count_target = []\n",
    "    neighbor_sim = []\n",
    "    clustering_coeff_source = []\n",
    "    clustering_coeff_target = []\n",
    "    graph_jaccoby_authors = []\n",
    "    graph_jaccard_coeff = []\n",
    "    pr_coeff_source = []\n",
    "    pr_coeff_target = []\n",
    "    \n",
    "    graph_ress_alloc = [p for u,v,p in ress_alloc]\n",
    "#     graph_soundarajan = [p for u,v,p in soundarajan]\n",
    "    \n",
    "    graph_adamic_index = [p for u,v,p in adamic_index]\n",
    "    graph_pref_attach = [p for u,v,p in pref_attach]\n",
    "    hubs_feature_source = []\n",
    "    hubs_feature_target = []\n",
    "    \n",
    "    graph_jaccard_coeff = [p for u,v,p in jaccard_coeff]\n",
    "    \n",
    "#     print np.size(graph_adamic_index)\n",
    "\n",
    "    \n",
    "    \n",
    "#     graph_cosine_authors = []\n",
    "\n",
    "    # we will use three basic features:\n",
    "\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    # tfidf similarities\n",
    "    tfidf_cos = []\n",
    "    \n",
    "#     topic model\n",
    "    topic_sim = []   \n",
    "    \n",
    "    # Journal matching\n",
    "    journal_matching = []\n",
    "    \n",
    "    # Important Topics\n",
    "#     U, S, V = np.linalg.svd(tfidf_mat.toarray())\n",
    "#     num_topics = 20\n",
    "#     Imp_topics = np.dot(A,V[:k,:].transpose())\n",
    "    \n",
    "    # Jaccard index\n",
    "    \n",
    "    # Cosine similarity\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     if tfidf_mat == None or topic_mat == None:\n",
    "#         corpus = [preprocess_string(element[5]) for element in node_info]\n",
    "        \n",
    "#         if tfidf_mat == None:\n",
    "#             vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "#             TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "#         else:\n",
    "#             TFIDF_matrix = tfidf_mat\n",
    "        \n",
    "#         if topic_mat == None:\n",
    "#             tm_vectorizer = text.CountVectorizer(stop_words='english', min_df=20)\n",
    "#             dtm = tm_vectorizer.fit_transform(corpus).toarray()\n",
    "    \n",
    "#             num_topics = 30\n",
    "#             clf = decomposition.NMF(n_components=num_topics)\n",
    "#             TOPIC_matrix = clf.fit_transform(dtm)\n",
    "#             TOPIC_matrix = TOPIC_matrix / np.sum(TOPIC_matrix, axis=1, keepdims=True)\n",
    "#         else:\n",
    "#             TOPIC_matrix = topic_mat                                    \n",
    "#     else:\n",
    "#         TFIDF_matrix = tfidf_mat\n",
    "#         TOPIC_matrix = topic_mat\n",
    "\n",
    "    counter = 0\n",
    "    for i in xrange(len(data_set)):\n",
    "        source = data_set[i][0]\n",
    "        target = data_set[i][1]\n",
    "    \n",
    "    \n",
    "        source_info = node_info[ID_pos[source]]\n",
    "        target_info = node_info[ID_pos[target]]\n",
    "    \n",
    "        # convert to lowercase and tokenize\n",
    "        source_title = source_info[2].lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        source_title = [token for token in source_title if token not in stpwds]\n",
    "        source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "        target_title = target_info[2].lower().split(\" \")\n",
    "        target_title = [token for token in target_title if token not in stpwds]\n",
    "        target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "        \n",
    "        \n",
    "        v1 = tfidf_mat[ID_pos[source],:].toarray()[0]\n",
    "        v2 = tfidf_mat[ID_pos[target],:].toarray()[0]        \n",
    "\n",
    "#         v1 = Imp_topics[ID_pos[source],:].toarray()[0]\n",
    "#         v2 = Imp_topics[ID_pos[target],:].toarray()[0]        \n",
    "\n",
    "        \n",
    "        temp_cosine = 0.0\n",
    "        \n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            temp_cosine = cosine(v1, v2)\n",
    "            \n",
    "        tfidf_cos.append(temp_cosine)\n",
    "        \n",
    "        \n",
    "        v1 = topic_mat[ID_pos[source],:]\n",
    "        v2 = topic_mat[ID_pos[target],:]\n",
    "        \n",
    "        topic_cosine = 0.0\n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            topic_cosine = cosine(v1, v2)         \n",
    "            \n",
    "        topic_sim.append(topic_cosine)\n",
    "        \n",
    "        \n",
    "        # Journal matching\n",
    "        source_journal = source_info[4]\n",
    "        target_journal = target_info[4]\n",
    "        \n",
    "        journal_match = 0\n",
    "        \n",
    "        if source_journal == str('') or target_journal == str(''):\n",
    "            journal_match = 0\n",
    "        elif source_journal == target_journal:\n",
    "            journal_match = 1\n",
    "        elif source_journal != target_journal:\n",
    "            journal_match = -1\n",
    "            \n",
    "        journal_matching.append(journal_match)\n",
    "            \n",
    "            \n",
    "        neighbors_source = len(list(graph.neighbors(source)))\n",
    "        neighbors_target = len(list(graph.neighbors(target)))\n",
    "        neighbor_count_source.append(neighbors_source)\n",
    "        neighbor_count_target.append(neighbors_target)\n",
    "        neighbors_source = set(list(graph.neighbors(source)))\n",
    "        neighbors_target = set(list(graph.neighbors(target)))\n",
    "        neighbor_sim.append(\n",
    "            len(neighbors_source.intersection(neighbors_target)))\n",
    "        clustering_coeff_source.append(clustering_coeff[source])\n",
    "        clustering_coeff_target.append(clustering_coeff[target])\n",
    "        authors_source = set(source_auth)\n",
    "        authors_target = set(target_auth)\n",
    "        graph_jaccoby_authors.append(float(len(authors_source.intersection(authors_target))) / float(len(authors_source.union(authors_target))))\n",
    "        pr_coeff_source.append(pr[source])\n",
    "        pr_coeff_target.append(pr[target])\n",
    "                \n",
    "        hubs_feature_source.append(hubs[source])\n",
    "        hubs_feature_target.append(hubs[target])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        \n",
    "        \n",
    "    \n",
    "        if counter % 10000 == 0:\n",
    "            print counter, \"training examples processsed\"\n",
    "        counter += 1\n",
    "        \n",
    "#     tfidf_source = vectorizer.fit(source_abstracts)._tfidf.idf_\n",
    "#     tfidf_target = vectorizer.fit(target_abstracts)._tfidf.idf_\n",
    "    \n",
    "\t\t\n",
    "    # convert list of lists into array\n",
    "    # documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "#     features = [overlap_title, temp_diff, comm_auth]\n",
    "#     if tfidf_mat is not None:\n",
    "#         features.append(tfidf_cos)\n",
    "#     if topic_mat is not None:\n",
    "#         features.append(topic_sim)\n",
    "        \n",
    "#     features = (np.array(features)).astype(float)\n",
    "\n",
    "    correlated_features_1 = (np.array([neighbor_count_source, pr_coeff_source, hubs_feature_source]).T).astype(float)\n",
    "    correlated_features_2 = (np.array([neighbor_count_target, pr_coeff_target, hubs_feature_target]).T).astype(float)\n",
    "    \n",
    "    _, _, v1 = svds(correlated_features_1, k=2)\n",
    "    _, _, v2 = svds(correlated_features_2, k=2)\n",
    "    \n",
    "    correlated_features_1 = np.dot(correlated_features_1, v1[:2,:].transpose())\n",
    "    correlated_features_2 = np.dot(correlated_features_2, v2[:2,:].transpose())\n",
    "    \n",
    "#     cor_feat_1_list = \n",
    "    \n",
    "    \n",
    "#     correl\n",
    "    \n",
    "#     correlated_features_3 = [\n",
    "    \n",
    "    features = (np.array([overlap_title, # 0\n",
    "                          temp_diff, # 1\n",
    "                          comm_auth, # 2\n",
    "                          tfidf_cos, # 3\n",
    "                          topic_sim, # 4\n",
    "#                           neighbor_count_source, # 5\n",
    "#                           neighbor_count_target, # 6\n",
    "                          neighbor_sim, # 7\n",
    "                          clustering_coeff_source, # 8\n",
    "                          clustering_coeff_target, # 9\n",
    "                          graph_jaccoby_authors, # 10\n",
    "#                           pr_coeff_source, # 11\n",
    "#                           pr_coeff_target, # 12\n",
    "                          graph_jaccard_coeff, # 13\n",
    "#                           graph_adamic_index, # 14\n",
    "                          graph_pref_attach, # 15\n",
    "                          graph_ress_alloc, # 16\n",
    "#                           hubs_feature_source, # 17\n",
    "#                           hubs_feature_target, # 18\n",
    "                          journal_matching # 21\n",
    "#                           graph_soundarajan\n",
    "                         ]).T).astype(float)\n",
    "    \n",
    "    features = np.concatenate((features,correlated_features_1, correlated_features_2),1)\n",
    "    \n",
    "    \n",
    "#     print features\n",
    "\n",
    "    # scale\n",
    "    features = preprocessing.scale(features)    \n",
    "    \n",
    "    if with_labels:\n",
    "\n",
    "        # convert labels into integers then into column array\n",
    "        labels = [int(element[2]) for element in data_set]\n",
    "        labels = list(labels)\n",
    "        labels_array = np.array(labels)\n",
    "        \n",
    "        return perm, data_set, features, labels_array, graph\n",
    "    else:\n",
    "        labels_array = np.zeros(1)\n",
    "        return perm, data_set, features, labels_array, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n",
      "210000 training examples processsed\n",
      "220000 training examples processsed\n",
      "230000 training examples processsed\n",
      "240000 training examples processsed\n",
      "250000 training examples processsed\n",
      "260000 training examples processsed\n",
      "270000 training examples processsed\n",
      "280000 training examples processsed\n",
      "290000 training examples processsed\n",
      "300000 training examples processsed\n",
      "310000 training examples processsed\n",
      "320000 training examples processsed\n",
      "330000 training examples processsed\n",
      "340000 training examples processsed\n",
      "350000 training examples processsed\n",
      "360000 training examples processsed\n",
      "370000 training examples processsed\n",
      "380000 training examples processsed\n",
      "390000 training examples processsed\n",
      "400000 training examples processsed\n",
      "410000 training examples processsed\n",
      "420000 training examples processsed\n",
      "430000 training examples processsed\n",
      "440000 training examples processsed\n",
      "450000 training examples processsed\n",
      "460000 training examples processsed\n",
      "470000 training examples processsed\n",
      "480000 training examples processsed\n",
      "490000 training examples processsed\n",
      "500000 training examples processsed\n",
      "510000 training examples processsed\n",
      "520000 training examples processsed\n",
      "530000 training examples processsed\n",
      "540000 training examples processsed\n",
      "550000 training examples processsed\n",
      "560000 training examples processsed\n",
      "570000 training examples processsed\n",
      "580000 training examples processsed\n",
      "590000 training examples processsed\n",
      "600000 training examples processsed\n",
      "610000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, training_dataset, training_features, training_labels, graph = get_features(\"../input/training_set.txt\",\n",
    "                                                                    \"../input/node_information.csv\",\n",
    "                                                                    1.0,\n",
    "                                                                    with_labels=True,\n",
    "                                                                    node_info = node_info,\n",
    "                                                                    tfidf_mat=TFIDF_matrix,\n",
    "                                                                    topic_mat=TOPIC_matrix)\n",
    "                                                                         #,topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, test_dataset, test_features, _, _= get_features(\"../input/testing_set.txt\", \"../input/node_information.csv\", 1.0,\n",
    "                                                  with_labels=False,\n",
    "                                                  node_info = node_info,\n",
    "                                                  tfidf_mat=TFIDF_matrix,\n",
    "                                                  topic_mat=TOPIC_matrix,\n",
    "                                                  graph = graph)\n",
    "#                                                   topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/training_feautres.csv\",training_features,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/test_features.csv\",test_features,delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"../output/training_labels.csv\",training_labels,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=300)\n",
    "classifier.fit(training_features, training_labels)\n",
    "pred=classifier.predict(test_features)\n",
    "results = np.concatenate((np.reshape(IDs,(np.size(pred), 1)), np.reshape(pred, (np.size(pred),1))), axis=1)\n",
    "with open(\"../input/cool_features_submission.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, )\n",
    "    writer.writerow(['id','prediction'])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [250, 300, 350],\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=classifier, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(training_features, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=CV_rfc.predict(test_features)\n",
    "results = np.concatenate((np.reshape(IDs,(np.size(pred), 1)), np.reshape(pred, (np.size(pred),1))), axis=1)\n",
    "with open(\"../input/dadadadada_submission.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, )\n",
    "    writer.writerow(['id','prediction'])\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d2c783770904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'bst:max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bst:eta'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'silent'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'objective'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'binary:logistic'\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "param = {'bst:max_depth':5, 'bst:eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "dtrain = xgb.DMatrix( training_features, label=training_labels)\n",
    "bst = xgb.train( param, dtrain, num_round, evallist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_features)\n",
    "pred = bst.predict(dtest)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

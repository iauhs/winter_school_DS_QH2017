{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from sklearn import decomposition\n",
    "import sklearn.feature_extraction.text as text\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_string(string):\n",
    "    th_regexp = re.compile('[a-zA-Z\\d]*-th')\n",
    "    citation_regexp = re.compile('[a-zA-Z]+[\\d]+')\n",
    "    digit_regexp = re.compile('[\\d]+')\n",
    "    double_whitespace_regexp = re.compile('\\s[\\s]+')\n",
    "    non_alphabet_regexp = re.compile('[\\W]')\n",
    "    one_letter_regexp = re.compile('\\s[a-zA-Z]\\s')\n",
    "    two_letters_regexp = re.compile('\\s[a-zA-Z][a-zA-Z]\\s')   \n",
    "    \n",
    "    \n",
    "    \n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "    \n",
    "    verbs_tags_set = set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "    \n",
    "    resulted_tokens = [token for (token, tag) in tagged_tokens if tag not in verbs_tags_set]\n",
    "    \n",
    "    for (token,tag) in tagged_tokens:\n",
    "        if tag not in verbs_tags_set:\n",
    "            resulted_tokens.append(token)\n",
    "            \n",
    "    result = \" \".join(resulted_tokens)\n",
    "\n",
    "\n",
    "    result = string\n",
    "    result = th_regexp.sub('', result)\n",
    "#     result = citation_regexp.sub('', result)\n",
    "    result = digit_regexp.sub('', result)\n",
    "    result = non_alphabet_regexp.sub(' ', result)\n",
    "    result = one_letter_regexp.sub(' ', result)\n",
    "    result = two_letters_regexp.sub(' ', result)\n",
    "    result = double_whitespace_regexp.sub(' ', result)    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info = list(reader)\n",
    "corpus = [preprocess_string(element[5]) for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75, ngram_range=(1,1))\n",
    "TFIDF_matrix = vectorizer.fit_transform(corpus)\n",
    "num_topics = 150\n",
    "_,_, V = svds(TFIDF_matrix, k=num_topics)\n",
    "TOPIC_matrix = TFIDF_matrix.dot(V.transpose())\n",
    "del V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# data loading and preprocessing \n",
    "\n",
    "# the columns of the data frame below are: \n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "\n",
    "def get_features(data_fname, info_fname, portion, with_labels=True, node_info = None, tfidf_mat=None,\n",
    "                 topic_mat=None, graph=None):\n",
    "    with open(data_fname, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data_set = list(reader)\n",
    "\n",
    "    data_set = [element[0].split(\" \") for element in data_set]\n",
    "\n",
    "    if portion == 1.0:\n",
    "        print \"one portion\", portion\n",
    "        to_keep = range(len(data_set))\n",
    "    else:                       \n",
    "        #to test code we select sample\n",
    "        print \"portion\", portion\n",
    "        random.seed(22)\n",
    "        to_keep = random.sample(range(len(data_set)), k=int(round(len(data_set)*portion)))            \n",
    "    \n",
    "    data_set = [data_set[i] for i in to_keep]\n",
    "    \n",
    "    perm = [i for i in to_keep]\n",
    "    \n",
    "    \n",
    "    valid_ids=set()\n",
    "    for element in data_set:\n",
    "        valid_ids.add(element[0])\n",
    "        valid_ids.add(element[1])\n",
    "        \n",
    "    if node_info is None:\n",
    "        tmp=[element for element in node_info if element[0] in valid_ids ]\n",
    "        node_info=tmp\n",
    "        del tmp\n",
    "\n",
    "\n",
    "\n",
    "    IDs = []\n",
    "    ID_pos={}\n",
    "    for element in node_info:\n",
    "        ID_pos[element[0]]=len(IDs)\n",
    "        IDs.append(element[0])\n",
    "        \n",
    "    print \"build graph\"\n",
    "    if graph is None:\n",
    "        graph = nx.Graph()\n",
    "        edges = []    \n",
    "        nodes = set()\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            nodes.add(source)\n",
    "            nodes.add(target)\n",
    "        \n",
    "            if with_labels:\n",
    "                if data_set[i][2] == \"1\":\n",
    "                    edges.append((source, target))\n",
    "            else:\n",
    "                edges.append((source, target))\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "        graph.add_nodes_from(nodes)\n",
    "        graph.add_edges_from(edges)\n",
    "    else:\n",
    "        all_edges = []\n",
    "        for i in xrange(len(data_set)):\n",
    "            source = data_set[i][0]\n",
    "            target = data_set[i][1]\n",
    "            all_edges.append((source, target))\n",
    "        \n",
    "    \n",
    "    clustering_coeff = nx.clustering(graph)\n",
    "    pr = nx.pagerank(graph, alpha=0.9)\n",
    "    jaccard_coeff = nx.jaccard_coefficient(graph, all_edges)\n",
    "    adamic_index = nx.adamic_adar_index(graph, all_edges)\n",
    "    pref_attach = nx.preferential_attachment(graph, all_edges)\n",
    "    ress_alloc = nx.resource_allocation_index(graph, all_edges)\n",
    "    hubs,_=nx.hits(graph,max_iter=1000)\n",
    "\n",
    "    # graph features\n",
    "    neighbor_count_source = []\n",
    "    neighbor_count_target = []\n",
    "    neighbor_sim = []\n",
    "    clustering_coeff_source = []\n",
    "    clustering_coeff_target = []\n",
    "    graph_jaccoby_authors = []\n",
    "    graph_jaccard_coeff = []\n",
    "    pr_coeff_source = []\n",
    "    pr_coeff_target = []\n",
    "    \n",
    "    graph_ress_alloc = [p for u,v,p in ress_alloc]\n",
    "    \n",
    "    graph_adamic_index = [p for u,v,p in adamic_index]\n",
    "    graph_pref_attach = [p for u,v,p in pref_attach]\n",
    "    hubs_feature_source = []\n",
    "    hubs_feature_target = []\n",
    "    \n",
    "    graph_jaccard_coeff = [p for u,v,p in jaccard_coeff]\n",
    "    \n",
    "    # we will use three basic features:\n",
    "\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    # tfidf similarities\n",
    "    tfidf_cos = []\n",
    "    \n",
    "#     topic model\n",
    "    topic_sim = []   \n",
    "    \n",
    "    # Journal matching\n",
    "    journal_matching = []\n",
    "    \n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    for i in xrange(len(data_set)):\n",
    "        source = data_set[i][0]\n",
    "        target = data_set[i][1]\n",
    "    \n",
    "    \n",
    "        source_info = node_info[ID_pos[source]]\n",
    "        target_info = node_info[ID_pos[target]]\n",
    "    \n",
    "        # convert to lowercase and tokenize\n",
    "        source_title = source_info[2].lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        source_title = [token for token in source_title if token not in stpwds]\n",
    "        source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "        target_title = target_info[2].lower().split(\" \")\n",
    "        target_title = [token for token in target_title if token not in stpwds]\n",
    "        target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "        \n",
    "        \n",
    "        v1 = tfidf_mat[ID_pos[source],:].toarray()[0]\n",
    "        v2 = tfidf_mat[ID_pos[target],:].toarray()[0]        \n",
    "\n",
    "        \n",
    "        temp_cosine = 0.0\n",
    "        \n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            temp_cosine = cosine(v1, v2)\n",
    "            \n",
    "        tfidf_cos.append(temp_cosine)\n",
    "        \n",
    "        \n",
    "        v1 = topic_mat[ID_pos[source],:]\n",
    "        v2 = topic_mat[ID_pos[target],:]\n",
    "        \n",
    "        topic_cosine = 0.0\n",
    "        if np.linalg.norm(v1)!= 0 and np.linalg.norm(v2) != 0 :\n",
    "            topic_cosine = cosine(v1, v2)         \n",
    "            \n",
    "        topic_sim.append(topic_cosine)\n",
    "        \n",
    "        \n",
    "        # Journal matching\n",
    "        source_journal = source_info[4]\n",
    "        target_journal = target_info[4]\n",
    "        \n",
    "        journal_match = 0\n",
    "        \n",
    "        if source_journal == str('') or target_journal == str(''):\n",
    "            journal_match = 0\n",
    "        elif source_journal == target_journal:\n",
    "            journal_match = 1\n",
    "        elif source_journal != target_journal:\n",
    "            journal_match = -1\n",
    "            \n",
    "        journal_matching.append(journal_match)\n",
    "            \n",
    "            \n",
    "        neighbors_source = len(list(graph.neighbors(source)))\n",
    "        neighbors_target = len(list(graph.neighbors(target)))\n",
    "        neighbor_count_source.append(neighbors_source)\n",
    "        neighbor_count_target.append(neighbors_target)\n",
    "        neighbors_source = set(list(graph.neighbors(source)))\n",
    "        neighbors_target = set(list(graph.neighbors(target)))\n",
    "        neighbor_sim.append(\n",
    "            len(neighbors_source.intersection(neighbors_target)))\n",
    "        clustering_coeff_source.append(clustering_coeff[source])\n",
    "        clustering_coeff_target.append(clustering_coeff[target])\n",
    "        authors_source = set(source_auth)\n",
    "        authors_target = set(target_auth)\n",
    "        graph_jaccoby_authors.append(float(len(authors_source.intersection(authors_target))) / float(len(authors_source.union(authors_target))))\n",
    "        pr_coeff_source.append(pr[source])\n",
    "        pr_coeff_target.append(pr[target])\n",
    "                \n",
    "        hubs_feature_source.append(hubs[source])\n",
    "        hubs_feature_target.append(hubs[target])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        \n",
    "        \n",
    "    \n",
    "        if counter % 10000 == 0:\n",
    "            print counter, \"training examples processsed\"\n",
    "        counter += 1\n",
    "    \n",
    "    features = (np.array([overlap_title, # 0\n",
    "                          temp_diff, # 1\n",
    "                          comm_auth, # 2\n",
    "                          tfidf_cos, # 3\n",
    "                          topic_sim, # 4\n",
    "                          neighbor_count_source, # 5\n",
    "                          neighbor_count_target, # 6\n",
    "                          neighbor_sim, # 7\n",
    "                          clustering_coeff_source, # 8\n",
    "                          clustering_coeff_target, # 9\n",
    "                          graph_jaccoby_authors, # 10\n",
    "                          pr_coeff_source, # 11\n",
    "                          pr_coeff_target, # 12\n",
    "                          graph_jaccard_coeff, # 13\n",
    "                          graph_adamic_index, # 14\n",
    "                          graph_pref_attach, # 15\n",
    "                          graph_ress_alloc, # 16\n",
    "                          hubs_feature_source, # 17\n",
    "                          hubs_feature_target, # 18\n",
    "                          journal_matching # 21\n",
    "                         ]).T).astype(float)\n",
    "#     print features\n",
    "\n",
    "    # scale\n",
    "    features = preprocessing.scale(features)    \n",
    "    \n",
    "    if with_labels:\n",
    "\n",
    "        # convert labels into integers then into column array\n",
    "        labels = [int(element[2]) for element in data_set]\n",
    "        labels = list(labels)\n",
    "        labels_array = np.array(labels)\n",
    "        \n",
    "        return perm, data_set, features, labels_array, graph\n",
    "    else:\n",
    "        labels_array = np.zeros(1)\n",
    "        return perm, data_set, features, labels_array, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n",
      "210000 training examples processsed\n",
      "220000 training examples processsed\n",
      "230000 training examples processsed\n",
      "240000 training examples processsed\n",
      "250000 training examples processsed\n",
      "260000 training examples processsed\n",
      "270000 training examples processsed\n",
      "280000 training examples processsed\n",
      "290000 training examples processsed\n",
      "300000 training examples processsed\n",
      "310000 training examples processsed\n",
      "320000 training examples processsed\n",
      "330000 training examples processsed\n",
      "340000 training examples processsed\n",
      "350000 training examples processsed\n",
      "360000 training examples processsed\n",
      "370000 training examples processsed\n",
      "380000 training examples processsed\n",
      "390000 training examples processsed\n",
      "400000 training examples processsed\n",
      "410000 training examples processsed\n",
      "420000 training examples processsed\n",
      "430000 training examples processsed\n",
      "440000 training examples processsed\n",
      "450000 training examples processsed\n",
      "460000 training examples processsed\n",
      "470000 training examples processsed\n",
      "480000 training examples processsed\n",
      "490000 training examples processsed\n",
      "500000 training examples processsed\n",
      "510000 training examples processsed\n",
      "520000 training examples processsed\n",
      "530000 training examples processsed\n",
      "540000 training examples processsed\n",
      "550000 training examples processsed\n",
      "560000 training examples processsed\n",
      "570000 training examples processsed\n",
      "580000 training examples processsed\n",
      "590000 training examples processsed\n",
      "600000 training examples processsed\n",
      "610000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, training_dataset, training_features, training_labels, graph = get_features(\"training_set.txt\",\n",
    "                                                                    \"node_information.csv\",\n",
    "                                                                    1.0,\n",
    "                                                                    with_labels=True,\n",
    "                                                                    node_info = node_info,\n",
    "                                                                    tfidf_mat=TFIDF_matrix,\n",
    "                                                                    topic_mat=TOPIC_matrix)\n",
    "                                                                         #,topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one portion 1.0\n",
      "build graph\n",
      "0 training examples processsed\n",
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "IDs, test_dataset, test_features, _, _= get_features(\"testing_set.txt\", \"node_information.csv\", 1.0,\n",
    "                                                  with_labels=False,\n",
    "                                                  node_info = node_info,\n",
    "                                                  tfidf_mat=TFIDF_matrix,\n",
    "                                                  topic_mat=TOPIC_matrix,\n",
    "                                                  graph = graph)\n",
    "#                                                   topic_mat=TOPIC_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "param = {'bst:max_depth':3, 'bst:eta':0.9, 'silent':1, 'objective':'binary:logistic', 'rate_drop' : 0.1 }\n",
    "dtrain = xgb.DMatrix( training_features, label=training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst = xgb.train( param, dtrain, 150 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_features)\n",
    "pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred2 = (pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.concatenate((np.reshape(IDs,(np.size(pred2), 1)), np.reshape(pred2, (np.size(pred2),1))), axis=1)\n",
    "with open(\"last_submission.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, )\n",
    "    writer.writerow(['id','prediction'])\n",
    "    writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
